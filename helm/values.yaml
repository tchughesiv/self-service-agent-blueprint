# Default values for self-service-agent
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# This will set the replicaset count more information can be found here: https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/
replicaCount: 1

# Global log level for all services (DEBUG, INFO, WARNING, ERROR)
logLevel: "INFO"

# This sets the container image more information can be found here: https://kubernetes.io/docs/concepts/containers/images/
image:
  registry: quay.io/ecosystem-appeng
  repository: self-service-agent
  requestManager: self-service-agent-request-manager
  agentService: self-service-agent-service
  integrationDispatcher: self-service-agent-integration-dispatcher
  mockEventing: self-service-agent-mock-eventing
  # This sets the pull policy for images.
  pullPolicy: Always
  # Overrides the image tag whose default is the chart appVersion.
  tag: "0.0.2"

# Test Integration Configuration
testIntegrationEnabled: false

# This is to override the chart name.
# nameOverride: ""
# fullnameOverride: ""

# Database Configuration
database:
  # Expected migration version that services should wait for
  expectedMigrationVersion: "003"
  
  # Database connection URL - leave empty to use default pgvector service
  # Format: postgresql://username:password@host:port/database
  url: ""  # Uses default: postgresql://postgres:changeme@self-service-agent-pgvector:5432/postgres

# This section builds out the service account more information can be found here: https://kubernetes.io/docs/concepts/security/service-accounts/
serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

# This is for setting Kubernetes Labels to a Pod.
# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
podLabels: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

# This is for setting up a service more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/
service:
  # This sets the service type more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
  type: ClusterIP
  # This sets the ports more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#field-spec-ports
  port: 8000


resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

# This section is for setting up autoscaling more information can be found here: https://kubernetes.io/docs/concepts/workloads/autoscaling/
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}

llama_stack_url: http://llamastack:8321


# PostgreSQL configuration for llama-stack agent persistence
pgvector:
  extraDatabases:
    - name: llama_agents
      vectordb: false
    - name: llama_responses
      vectordb: false

# Llama Stack agents configuration with PostgreSQL persistence
llama-stack:
  initContainers:
    enabled: true
  resources: {}
  providers:
    agents:
      - provider_id: meta-ref-postgres
        provider_type: inline::meta-reference
        config:
          persistence_store:
            type: postgres
            namespace: null
            host: ${env.POSTGRES_HOST:=pgvector}
            port: ${env.POSTGRES_PORT:=5432}
            db: llama_agents
            user: ${env.POSTGRES_USER:=pgvector}
            password: ${env.POSTGRES_PASSWORD:=pgvector}
          responses_store:
            type: postgres
            host: ${env.POSTGRES_HOST:=pgvector}
            port: ${env.POSTGRES_PORT:=5432}
            db: llama_responses
            user: ${env.POSTGRES_USER:=pgvector}
            password: ${env.POSTGRES_PASSWORD:=pgvector}

# Slack integration is now handled by the integration-dispatcher
# Configuration is in the requestManagement section below
slack:
  enabled: true
  botToken: ""
  signingSecret: ""
  service:
    type: ClusterIP
    port: 8080
  route:
    enabled: true
    host: ""
    tls:
      enabled: true
      termination: edge

mcp-servers:
  mcp-servers:
    self-service-agent-snow:
      deploy: true
      imageRepository: quay.io/ecosystem-appeng/self-service-agent-snow-mcp
      pullPolicy: Always
      uri: http://self-service-agent-snow:8000/sse/
      imageTag: 0.0.2
      env:
        SERVICENOW_AUTH_TYPE: "basic"
        USE_REAL_SERVICENOW: "false"

    mcp-weather:
      mcpserver:
        enabled: false

    oracle-sqlcl:
      mcpserver:
        enabled: false

# Enable/disable the llm-service dependency
llm-service:
  enabled: true

# Request Management Layer Configuration
requestManagement:
  enabled: true
  
  # External Access Configuration
  externalAccess:
    enabled: true  # Set to true to create external access via OpenShift Routes
    # When disabled, services are only accessible within the cluster
    
  # Knative Eventing Configuration (for event-driven architecture)
  knative:
    enabled: true
    eventing:
      enabled: false  # Set to true to enable Knative eventing (production mode)
    broker:
      name: "self-service-agent-broker"
      # Centralized broker URL configuration
      url: "http://kafka-broker-ingress.knative-eventing.svc.cluster.local"
      config:
        # Kafka Broker configuration
        numPartitions: 3
        replicationFactor: 1
        retentionDuration: P7D
    
    # Mock Eventing Service (default eventing mode for development and testing)
    mockEventing:
      enabled: true  # Default to mock eventing service (disable for full Knative eventing)
      logLevel: "INFO"
      resources: {}
      # Health check configuration (dev-optimized for resource-constrained environments)
      healthChecks:
        livenessProbe:
          initialDelaySeconds: 15     # Conservative for dev infrastructure
          periodSeconds: 30           # Reasonable check frequency
          timeoutSeconds: 10          # Longer timeout for slower dev nodes
          failureThreshold: 5         # More failures allowed for dev instability
          successThreshold: 1
        readinessProbe:
          initialDelaySeconds: 5      # Conservative for dev infrastructure
          periodSeconds: 15           # Less frequent to reduce load
          timeoutSeconds: 5           # Longer timeout for slower dev nodes
          failureThreshold: 5         # More failures allowed for dev instability
          successThreshold: 1

    # KnativeKafka configuration (automatically enabled when eventing is enabled)
    kafka:
      # enabled is automatically set to true when eventing.enabled is true
      # name: "self-service-agent-kafka-eventing"  # Defaults to include release namespace
      replicas: 1
      logLevel: "INFO"
      
      
      # Broker configuration (Knative Kafka Broker - enables Kafka broker support)
      broker:
        numPartitions: 10
      
      
      # Resource limits for KnativeKafka components
      resources: {}
      
      # Security configuration
      security:
        protocol: "PLAINTEXT"  # or "SASL_PLAINTEXT", "SSL", "SASL_SSL"
        sasl:
          enabled: false
          mechanism: "PLAIN"  # or "SCRAM-SHA-256", "SCRAM-SHA-512"
          user: ""
          secretName: ""
          secretKey: ""
  
  # Kafka cluster configuration for KnativeKafka channels
  kafka:
    enabled: true
    name: "self-service-agent-kafka"
    replicas: 1
    storage:
      type: "ephemeral"  # Use "persistent-claim" for production
      size: "10Gi"       # Only used if type is "persistent-claim"
    config:
      defaultReplicationFactor: 1
      minInSyncReplicas: 1
      offsetsTopicReplicationFactor: 1
      transactionStateLogMinIsr: 1
      transactionStateLogReplicationFactor: 1
    resources: {}
    entityOperator:
      topicOperator:
        resources: {}
      userOperator:
        resources: {}

  
  # Network Policies Configuration
  networkPolicies:
    enabled: true  # Enable network policies for cross-namespace communication
    additionalIngressRules: []  # Additional ingress rules if needed
  
  # Request Manager Service
  requestManager:
    replicas: 1
    resources: {}
    # Health check configuration (dev-optimized for resource-constrained environments)
    healthChecks:
      livenessProbe:
        initialDelaySeconds: 20     # Conservative for dev infrastructure
        periodSeconds: 30           # Reasonable check frequency
        timeoutSeconds: 10          # Longer timeout for slower dev nodes
        failureThreshold: 5         # More failures allowed for dev instability
        successThreshold: 1
      readinessProbe:
        initialDelaySeconds: 10     # Conservative for dev infrastructure
        periodSeconds: 15           # Less frequent to reduce load
        timeoutSeconds: 5           # Longer timeout for slower dev nodes
        failureThreshold: 5         # More failures allowed for dev instability
        successThreshold: 1
      startupProbe:
        initialDelaySeconds: 5      # Conservative start for dev
        periodSeconds: 5            # Less frequent checks for dev
        timeoutSeconds: 10          # Longer timeout for slower dev nodes
        failureThreshold: 24        # 2 minutes total startup time (dev infrastructure)
        successThreshold: 1
    # HPA-style autoscaling
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 10
      targetCPUUtilization: 70
      targetMemoryUtilization: 80
  
  # Integration Dispatcher Service
  integrationDispatcher:
    replicas: 1
    resources: {}
    # Health check configuration (dev-optimized for resource-constrained environments)
    healthChecks:
      livenessProbe:
        initialDelaySeconds: 15     # Conservative for dev infrastructure
        periodSeconds: 30           # Standard frequency
        timeoutSeconds: 10          # Longer timeout for slower dev nodes
        failureThreshold: 5         # More failures allowed for dev instability
        successThreshold: 1
      readinessProbe:
        initialDelaySeconds: 8      # Conservative for dev infrastructure
        periodSeconds: 15           # Less frequent to reduce load
        timeoutSeconds: 5           # Longer timeout for slower dev nodes
        failureThreshold: 5         # More failures allowed for dev instability
        successThreshold: 1
      startupProbe:
        initialDelaySeconds: 3      # Conservative start for dev
        periodSeconds: 5            # Less frequent checks for dev
        timeoutSeconds: 10          # Longer timeout for slower dev nodes
        failureThreshold: 20        # 1.5 minutes total startup time (dev infrastructure)
        successThreshold: 1
    # HPA-style autoscaling
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 5
      targetCPUUtilization: 70
      targetMemoryUtilization: 80
  
  # Agent Configuration
  agent:
    # Default agent ID for routing (can be overridden)
    defaultAgentId: "routing-agent"
    # Agent response timeout in seconds
    timeout: 120
    # Always refresh agent mapping from LlamaStack on each request
    # Set to false for better performance, true for maximum reliability
    alwaysRefreshMapping: true

  # Agent Service
  agentService:
    replicas: 1
    resources: {}
    # Health check configuration (dev-optimized for resource-constrained environments)
    healthChecks:
      livenessProbe:
        initialDelaySeconds: 25     # Conservative for dev infrastructure (LLM loading)
        periodSeconds: 30           # Standard frequency
        timeoutSeconds: 15          # Longer timeout for LLM operations on dev nodes
        failureThreshold: 5         # More failures allowed for dev instability
        successThreshold: 1
      readinessProbe:
        initialDelaySeconds: 15     # Conservative for dev infrastructure (LLM loading)
        periodSeconds: 15           # Less frequent to reduce load
        timeoutSeconds: 10          # Longer timeout for LLM operations on dev nodes
        failureThreshold: 5         # More failures allowed for dev instability
        successThreshold: 1
      startupProbe:
        initialDelaySeconds: 5      # Conservative start for dev
        periodSeconds: 5            # Less frequent checks for dev
        timeoutSeconds: 15          # Longer timeout for LLM operations on dev nodes
        failureThreshold: 30        # 2.5 minutes total startup time (dev infrastructure + LLM)
        successThreshold: 1
    # HPA-style autoscaling
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 5
      targetCPUUtilization: 70
      targetMemoryUtilization: 80
  
  # Init Job
  initJob:
    resources: {}
  
  # Integration Configuration
  integrations:
    # Service-level configuration
    services:
      email:
        enabled: false
        smtpHost: "smtp.example.com"
        smtpPort: "587"
        smtpUseTls: "true"
        fromEmail: "noreply@selfservice.local"
        fromName: "Self-Service Agent"
      slack:
        enabled: false
        # Slack credentials loaded from secrets
    
    # User defaults (auto-enabled based on service configuration)
    userDefaults:
      SLACK:
        # Allow Slack to be enabled when health check passes
        enabled: true
        priority: 1
        retryCount: 3
        retryDelaySeconds: 60
      EMAIL:
        # Allow Email to be enabled when health check passes
        enabled: true
        priority: 2
        retryCount: 3
        retryDelaySeconds: 60
      WEBHOOK:
        # Webhook controlled by URL presence - no enabled flag needed
        # url: "https://your-webhook-endpoint.com/webhook"
        priority: 3
        retryCount: 1
        retryDelaySeconds: 30
      TEST:
        # Allow Test integration to be enabled when health check passes
        enabled: true
        priority: 5
        retryCount: 1
        retryDelaySeconds: 10
  
  # Database Migration
  dbMigration:
    enabled: true
    resources: {}

# Security Configuration
security:
  # API Keys for tool integrations
  apiKeys:
    # Tool API keys (for external integrations)
    snowIntegration: ""
    hrSystem: ""
    monitoringSystem: ""
    # Web API keys for testing and internal tools
    webKeys:
      # Format: "key-name": "user-email"
      "web-test-user": "test@company.com"
      "web-admin": "admin@company.com"
      "web-demo": "demo@company.com"
  
  # Slack configuration
  slack:
    signingSecret: ""
    botToken: ""  # For Integration Dispatcher
  
  # JWT Authentication Configuration
  jwt:
    enabled: false  # Set to true to enable JWT validation
    issuers:
      # Example: Red Hat SSO
      - issuer: "https://sso.redhat.com/auth/realms/redhat-external"
        jwksUri: "https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/certs"
        audience: "selfservice-api"
        algorithms: ["RS256"]
      # Example: Custom OIDC provider
      - issuer: "https://auth.company.com"
        jwksUri: "https://auth.company.com/.well-known/jwks.json"
        audience: "selfservice-api"
        algorithms: ["RS256", "HS256"]
    validation:
      verifySignature: true
      verifyExpiration: true
      verifyAudience: true
      verifyIssuer: true
      leeway: 60  # Seconds of leeway for clock skew

  # Email configuration for Integration Dispatcher
  email:
    smtpHost: ""
    smtpPort: "587"
    smtpUsername: ""
    smtpPassword: ""
    smtpUseTls: "true"
    fromEmail: "noreply@selfservice.local"
    fromName: "Self-Service Agent"
